# StatefulSet

개요 : Application 종류에는 Statless, Stateful 두 가지가 있다.

<img src=../image/hpa.PNGstatefulset1.PNG>

Stateless Application의 대표적인 예는 Web Server이다.
Stateful Applicaion의 대표적인 예는 Database이다.

두 Application간의 특징을 살펴보자.

Stateless는 app이 여러 개 배포되더라도 모두 똑같은 서비스 역할을 한다.
Stateful는 각각의 app마다 자신의 역할이 있다.

IF ) App3이 죽는다는 가정.
Stateless는 App4로 단순히 서비스를 복제해주면 되는데,
Stateful는 Arbiter의 역할을 하는 App3이 죽는다면, 반드시 Arbitier의 역할을 하는 app이 만들어줘야 하고, 이름도 App3로 같아야 한다.

Stateless app은 볼륨이 반드시 필요하지 않지만, 만약 app의 로그를 영구적으로 저장하고 싶을 때는 볼륨과 연결할 수 있고, 모든 app을 하나의 볼륨의 연결할 수도 있다.

반면, Statefull app은 각각의 역할이 다른 만큼 볼륨도 각각 써야한다.

app들에 연결이 되는 대상과 네트워킹 흐름에 대한 차이도 있는데, Stateless 대체로 사용자들이 접속을 하고, 이 접속된 네트워크 트래픽은 한 web에만 집중이 되면 서버에 문제가 생기기 때문에 그렇지 않도록 여러 web에 분산이 되는 형태로 흘러가게 된다.

Stateful app은 대체로 내부 시스템들이 Database에 저장을 하도록 연결을 하고, 이때 이 트래픽은 각 app의 특징에 맞게 들어와야 한다.

정리하자면, stateful app은 역할에 따라 load가 있는 형태이고, stateless app은 단순 분산 목적을 가진 연결이다.

<img src=../image/hpa.PNGstatefulset2.PNG>
이러한 특징을 가지는 app들을 위해 쿠버네티스에도 활용할 수 있게 해주는 컨틀롤러가 있는데,
stateless app은 ReplicaSet, stateful app은 StatefulSet 컨트롤러이다.

## StatefulSet Detail

<img src=../image/hpa.PNGstatefulset3.PNG>

statefulset은 controller이며, replicaset과 비교해서 설명을 할거임.

`replicas=1`로 pod가 1개 생성되는 것은 동일하나, replicaset의 pod는 랜덤 이름이고,
statefulset의 pod 이름은 0 부터 순차적으로 숫자가 붙어서 생성이 된다.

`repicas=0`으로 하게 되면, replicaset의 pod는 동시에 삭제 되지만,
statefulset의 pod는 높은 숫자부터 순차적으로 삭제 된다.

## PVC, Headless Service 연결

<img src=../image/hpa.PNGstatefulset4.PNG>

replicaset은 pod에 볼륨을 연결하려면 pvc를 별도로 생성을 해야 하고, pod 템플릿에 pvc를 지정을 하기 때문에 바로 연결이 된다.
statefulset의 경우, 템플릿에 통해 파드가 만들어지고, 추가적으로 볼륨 클레임 템플릿이 있는데, 이것을 통해 pvc가 동적으로 생성이 되고 pod와 자동으로 연결이 된다.

`replicas=3`로 변경하면, replicaset은 pod들이 모두 pvc로 연결이 된다.
statefulset은 볼륨 클라임 템플릿에 의해서 pvc가 각각 연결된다.

주의할 점은 replicaset은 pvc가 노드1에 만들어졌다면, pvc에 연결하는 파드도 노드1에 있어야 한다. 그렇기 떄문에 pod template에 `nodeSelector:`로 지정해줘야 한다.

statefulset은 동적으로 pod와 pvc가 같은 노드에 만들어지기 때문에 알아서 모든 노드에 균등하게 배포된다.

`replicas=0`으로 줄이면, statefulset은 pod는 순차적으로 지우지만 pvc는 따로 지우지 않는다. 사용자가 직접 삭제해야함.

statefulset을 만들 때 `ServiceName: "Headless"`를 사용해서 만들게 되면, pod에 예측 가능한 도메인 이름이 만들어지기 때문에, 인터널 서버의 특정 파드입장에서 원하는 statefulset의 파드와 연결할 수가 있다.

# Ingress

사용 목적 : Service LoadBalancing, Canary Upgrade

<img src=../image/ingress1.PNG>

쇼핑몰을 운영한다고 가정했을 때, Service LoadBalancing은 쇼핑 페이지, 고객 센터, 주문 서비스를 파드별로 각각 만들어, 하나의 파득 문제가 생겼을 때 다른 app은 문제가 되지 않도록 할 수 있다. 이러한 각 app에 외부에서 접근을 하기 위해서 서비스를 달고, 특정 도메인으로 접근을 할 때 네트워크 상에서 L4, L7과 같은 장비가 필요한데, K8S에선 Ingress가 그 역할을 한다.

<img src=../image/ingress2.PNG>

version1의 app들이 구동되서 서비스가 되고 있는 상태에서 테스트할 version2에 app을 구동시키고 ingress를 만들어서 이 두 버전의 서비스를 연결을 하면, 사용자가 ingress를 통해 접근을 했을 때 10% 일부 트래픽만 v2에 가게 할 수 있다.

## Ingress Detail

<img src=../image/ingress3.PNG>

Ingress라는 오브젝트는 k8s 설치가 되어있으면 바로 만들 수가 있다.
Ingress에는 `Host`로 도메인 이름을 넣을 수 있고, 이 도메인으로 들어오는 트래픽은 `Path`에 따라 원하는 서비스(`serviceName`)로 연결을 하라는 것이 주 설정 내용이다.

Ingress를 만들어도 작동 되는 것이 아무것도 없다. 아직 Rule을 실행할 구현체가 없기 때문인데 k8s에서 이 구현체를 만들기 위해서는 플러그인을 깔아야 한다.

## Ingress Controller

<img src=../image/ingress4.PNG>

대표적으로 nginx와 kong이 존재한다. 그 외 많은 컨트롤러도 존재.

nginx를 설치하게 되면, nginx에 대한 NS가 생기고 그 위에 Deployment와 ReplicaSet이 만들어지면서, 실제 Ingress의 구현체인 nginx pod가 만들어진다. pod가 ingress rule이 있는지 보고 있다면 rule에 따라 서비스에 연결을 해주는 역할을 한다.
이 룰에 따라 트래픽이 해당 서비스에 전달이 되려면 외부에서 접근하는 사용자들의 트래픽은 nginx 파드를 지나야 하기 때문에 외부에서 접근할 수 있는 service를 하나 만들어서 연결을 해줘야 한다.

ingress를 더 추가할 수도 있는데, 다른 도메인을 주고 `Path` 없이 바로 서비스에 지정할 수 있다.

## Ingress 기능

### Service Loadbalancing

---

<img src=../image/ingress5.PNG>

환경 설정:
각각의 업무 별로 pod와 svc 만들고, 사전에 nginx controller가 설치가 됐고, 파드가 외부와 연결이 되도록 NodePort 서비스도 연결을 했기 때문에 192.168.0.30:30431로 접근을 하면 트래픽이 전송이 되는 환경으로 구성됨.

이러한 환경에서 Ingress를 만들고 규칙을 각 path에 따라 의도하는 서비스로 매칭하면 준비가 끝난다.

사용자는 도메인에 따라서 각 서비스로 접근이 가능하다.

### Canary Update

---

<img src=../image/ingress6.PNG>

가정 환경:
www.app.com으로 사용자가 접근을 하면, svc-v1이라는 서비스와 연결이 되는 환경을 구성되었고, 현재 사용자들에게 app이 운영되고 있는 상황임.

이 환경에서 canary upgrade를 테스트할 파드와 서비스를 띄우고 ingress를 하나 더 만드는데, hostname은 www.app.com으로 똑같고, serviceName을 svc-v2로 설정.

새로 만든 ingress에 weight:10%라는 옵션을 사용하면, 트래픽이 10%만 전송된다.
또한, 특정 언어별로 테스트를 하고 싶을 때 header 옵션을 이용할 수도 있다.

### Https

---

<img src=../image/ingress7.PNG>

https로 연결을 할 수 있도록 인증서 관리도 할 수 있다.
파드 자체에서 인증서 기능을 제공하기 힘들 때 사용하면 좋다.

ingress를 만들 때, 도메인과, svc와 연결을 하고, `tls`라는 옵션에 `secretName`으로 실제 secret 오브젝트를 연결을 하고, secret 안에는 데이터 값으로 인증서를 담고 있는데, 이렇게 구성을 하면, 사용자가 도메인 이름 앞에 https을 붙여야만 접근이 가능하다.

# Autoscaler

k8s에서 Autoscaler에는 3가지 종류가 있다.
pod의 개수를 늘리는 HPA와, pod의 리소스를 증가시키는 VPA, 클러스터의 노드를 추가하는 CA가 있다.

## HPA

<img src=../image/hpa1.PNG>

가정 환경:
controller가 있고, replicas=1 에 의해 pod 하나가 생성되어 구동중임.
또한, 서비스도 연결이 되어, 모든 트래픽인 pod에 전송되는 상황임.

트래픽이 많아져서 어느 순간 pod내의 리소스를 사용하는 상황이고, 트래픽이 좀 더 많아지면 pod가 다운이 되는 상황이다.

사전에 HPA를 만들고 컨틀롤러에 연결을 해놓았다면, HPA가 파드의 리소스 상태를 감지하고 있다가, 이러한 위험한 상황이 오게되면 controller의 replicas를 증가시킨다.

이렇게 되면 controller는 pod를 하나 더 생성하게 되고, 파드는 수평적으로 증가하게 되는데, Scale Out이라고 하고, 반대로 트래픽이 감소해 리소스 사용량이 감소하면 파드가 삭제가 되는데, Scale In이라고 한다.

권장되는 조건:

- 기동이 빠르게 되는 App
- Stateless App
  stateful app은 각 파드마다 역할이 있는데, HPA는 각 파드의 역할을 알지못하기 때문에 어떤 파드를 생성하고 삭제해야하는지 모른다.

## VPA

<img src=../image/hpa2.PNG>

가정 환경:
controller가 있고, replicas=1 에 의해 pod 하나가 생성되어 구동중임.
파드는 메모리가 1G, cpu가 1core로 설정이 되어 있음.

리소스 자원이 모두 사용되는 상황이 왔을 때,
사전에 VPA를 controller에 연결을 해놓았다면, VPA가 파드의 리소스 상태를 감지하고 있다가, 상태를 인지하고, 파드를 restart 시키면서 리소스를 증가시킨다. 이렇게 리소스의 양이 수직적으로 증가하는 것이, Scale UP이라고 하고 반대로 감소하는 것이 Scale Down이다.

권장되는 조건:

- Stateful App
- 한 Controller에 HPA와 함께 사용안됨.

## CA

<img src=../image/hpa3.PNG>

클러스터에 있는 모든 노드에 자원이 없을 경우 동적으로 worker 노드를 추가시켜준다.

가정 환경:
노드 2개가 있고, pod들이 운영되고 있다. pod를 만들면 스케쥴러가 할당해주는 노드에 배치가 되는 환경임.

어느 순간 노드들의 자원들이 모두 소모가 됨. 이 상태에서 파드를 하나 더 생성하려고 했을 때 스케쥴러는 더이상 어느 노드에도 파드를 배치할 수 없다는 것을 확인을 하고, CA한테 worker노드를 하나 더 생성해달라고 요청을 한다.

만약 CA를 사전에 특정 Cloud Provider와 연결을 해놓았다면, 요청이 들어 왔을 때 해당 porvider에 노드를 하나 더 만들어 주고, 스케쥴러는 생성된 provider에 파드를 생성한다.

운영이 되다가 기존에 사용하는 파드들이 없어져서 node들의 자원이 남게되면, 스케쥴러는 이것을 파악하고, CA에게 Cloud Provider에 있는 노드를 삭제해달라고 요청을 한다. 이후 노드가 삭제가 되면 위에서 돌고 있던 파드는 기존 노드에 배치가 된다.

## HPA Architecture

<img src=../image/hpa4.PNG>

Master Node에 Control Plane Component라고 해서 k8s에서 주요 기능을 하는 Component들이 pod형태로 해서 돌아가고 있는데, Controller Manager는 다양한 controller들이 Thread 형태로 돌아가고 있다.
api-server가 있는데, 이건 k8s 노드 통신의 길목형태라고 생각하면 된다. 사용자가 k8s 접근을 했을 때도 이용을 하지만, k8s내의 component들 조차도 db의 접근을 한다거나, 타 component들을 호출을 할 때 사용한다.

Worker Node Component라고 해서, K8S 설치 시 각 노드에 kubelet이 설치가 되고, node를 대표하는 agent 역할을 하는데, 자신의 노드에 있는 pod를 관리하는 역할을 한다.

이렇다 해도 kubelet이 직접 컨테이너를 만드는 것은 아니고, Controller Runtime이라고 해서 실제 컨테이너를 생성하고, 설치하는 구현체가 있다.

여기까지 설명한 것 토대로 사용자가 replicaset을 만들었을 때 과정:
replicaset을 담당하는 Thread는 replicas=1 이라고 했을 때 pod를 하나 만들어 달라고, kube-apiserver를 통해 wokrer node의 kubelet에 요청을 한다.
kubelet은 pod는 k8s 개념이고, 이 안에 컨테이너만 빼서 도커에게 만들어 달라고 요청하고, 그럼 도커가 노드 위에 컨테이너를 만들어준다.

Resource Estimator인 cAdvisor가 docker로부터 memory와 cpu의 대한 성능 정보를 측정하는데, 이 정보를 kubelet에 통해 가져갈 수 있도록 설정을 함.

Addon Component로 metric Server를 설치하고, 각 노드에 있는 kubelet한테, mem과 cpu 정보를 가지고 와서 저장을 해놓고, 이 데이터들을 다른 component들이 사용하도록 kube-apiserver에 등록을 해놓는다.

이렇게 되면 hpa가 cpu와 mem 정보를 kube-apiserver를 통해 가져갈 수 있게 되고, hpa는 15초 마다 체크를 하고 있다가, pod의 리소스 사용량이 높아졌을 때 replicas의 값을 증가시킨다.

그리고 `kubectl top` 명령어로 통해 이 resource api를 통해서 pod나 node의 현재 리소스 상태를 조회할 수 있고, 추가적으로 prometheus를 설치하면 단순 메모리나, cpu 외에도 다양한 metric 정보를 수집할 수가 있는데, pod로 들어오는 패킷 수나, ingress를 통해 들어오는 request 양들에 대한 metirc 등 다양한 정보를 수집할 수 있다. hpa는 이 정보를 trigger로도 controller의 replicas를 조절할 수 있다.

> 이를 통해, 간략한 k8s architecture와 hpa가 동작하는 원리, metric server를 추가해야 하는 원리를 알아 볼 수 있다.

## HPA Detail

<img src=../image/hpa5.PNG>

가정 환경:
Deployment로 `replicas=2`로 설정하면 replicaset이 만들어지고 pod도 2개 만들어진다.
그리고 pod의 resource 값이 설정이 된 상태임.

이러한 환경에서, HPA를 만든다면??

설정으로는 controller를 지정하는 `target`부분과, 증감되는 replicas의 값을 지정해주는 `maxReplicas`, `minReplicas`가 있다. 다음으로, `metrics`는 metric정보에 어떤 조건을 통해서 replicas를 증가시킬지에 대한 부분인데, type이라고 해서 resource로 선택을 하면, pod의 resource 부분을 가리키는 것이다. 세부적으로 name의 cpu, mem을 볼 것인지, 정할 수 있고, 어떤 조건으로 replicas를 증가시킬지에 대한 부분은 가장 기본으로 사용되는 옵션인 `Utilization`이 있고, 평균을 `50%`라고 했다면, 파드의 request값 기준으로 현재 사용 자원이 50% 이상이면 replicas를 증가시킨다.

하지만, 이 수치를 넘긴다고 무조건 pod를 하나씩 생성하는 것이 아니고, 정해진 공식을 가지고, 한번에 몇개를 증가시킬지 결정을 한다.

계산 예시)
resource를 cpu로 정하고, 두 파드의 request cpu는 200이고, 평균 Utilization을 50으로 잡아 놓았다면, 실제 cpu 사용량이 100을 넘게되면 hpa는 replicas를 증가시키게 된다.

scaleout이 되는 예로, 현재 평균 cpu가 300이면,

```
[ 현재 replicas ] x [ 평균 cpu ] / [ target 값 ] = [ 증가 시킬 replicas ]
```

scalein이 되는 상황에도 똑같이 위 공식을 이용한다.

`type`의 종류에는 %가 아닌 실제 평균 수치를 넣는 `AverageValue`와 `Value`가 있다.

`metrics`의 `type`에는 파드와 서비스와 인그레스가 달려있다고 가정했을 때,
metrics type이 `Pods`라면 pod에 들어오는 패킷수나, 파드와 관련된 정보로 scale in,out을 할 수 있고, pod외에 인그레스와 같은 다른 오브젝트에 대한 메트릭 정보는 `Object` `type`으로 지정하면 된다.
