# 1. Service

초급편에선, 사용자의 입장에서 서비스를 다루었지만, 중급편에선, Pod의 입장에서 다룰 것이다.

## 개요

<img src=../image/service1.PNG>

사용자의 접근의 경우 서비스가 만들어진 후에 IP를 확인하고 그 IP로 접근을 하면되는데,

Pod의 경우 이 자원들이 동시에 배포가 될 수 있다.  
이럴 때 Pod A가 Pod B로 접근을 해야하는데, 파드b의 ip를 넣기에는 ip는 동적으로 할당되기 때문에, 미리 알 수가 없다.  
또한, 파드 b가 문제가 생겨 재생성이 되면, ip는 계속 바뀌기 때문에 파드 a가 지속적으로 쓸 수가 없다.

-> 이러한 문제를 해결하고, 파드 a가 파드 b와 서비스의 연결을 하기 위해서는 `Headless`,
`DNS Server`가 필요하다.

파드가 외부 특정 사이트에 접근을 해서 데이터를 가지고 오는 상황에서 접근 주소를 변경해야 하는 상황이 생기면 파드를 변경하고 재배포를 해야할까??

-> Service의 `ExternalName`을 이용해서 외부 연결을 파드의 수정없이 할 수 있다.

<img src=../image/service1-1.PNG>

k8s cluster안에는 DNS Server가 별도로 존재하는데, 서비스의 도메인 이름과, ip가 저장되어 있다. 파드가 서버1에 대한 도메인 이름을 지정하면 해당 ip를 알려준다.

내부망에서도 DNS Server가 구축되어 있다면, 내부 서버들이 생겼을 때 해당 이름들이 DNS 등록이 되었을 거고,
Pod가 user1을 찾았을 때 K8S의 DNS 없다면, DNS 특성상 상위 DNS를 찾게되고 해당 IP를 알려준다.

외부에 있는 사이트도 도메인 이름이 외부 도메인에 등록되어 있기 때문에 Pod가 Google을 찾으면 부모의 부모를 찾아서 최종적으로 Google의 ip를 찾게 된다.

Pod에서 DNS를 이용해서 원하는 서비스나 외부에 접근을 할 수가 있다.
그래서 파드가 IP를 몰라도 DNS의 서비스 이름으로 IP를 물어봐서 연결을 할 수가 있다.

Pod1이나 Pod2를 선택해서 연결을 하고 싶을 때면 pod에 headless 서비스를 연결을 하면 된다. -> DNS Server에 파드의 이름과 서비스의 이름이 붙여줘서 도메인 이름으로 등록되기 때문에, 파드3의 입장에서 파드1에 접근을 하기 위해서는 IP주소는 필요가 없고, 도메인 이름으로 접근을 할 수가 있다.

`ExternalName` 서비스를 만들어서, 특정 외부 도메인을 넣을 수가 있는데, DNS를 타고 타서 Google ip를 가져올 수 있고, 파드는 이 서비스를 통해서 데이터를 가져오도록 해놓으면, 추후 데이터를 github에서 가져오도록 변경을 했을 때 파드의 수정없이 서비스의 `ExternalName`만 변경하면 된다.

## 1-1. Headless

---

<img src=../image/service1-2.PNG>

- default NS에 파드 두 개와 서비스가 연결되어 있음. (ClusterIP)
- pod는 pod1에 연결하고 싶음
- K8S DNS가 있는데 이름은 cluster.local이고, 파드든 서비스든 긴 도메인 이름과 ip가 저장이 된다.
- pod는 clusterip를 통해 pod1과 연결을 할 수가 있다.

<img src=../image/service1-3.PNG>

<DNS 테이블>

Service : 서비스명.NS.svc.DNS명  
Pod : 파드IP,NS,pod,DNS명

-> 이러한 형식을 FQDN이라고 한다. 같은 NS 안에서는 Service는 앞자리만 짧게 써도 되지만, 파드는 모든 것을 입력해야 한다.

---

<img src=../image/service1-4.PNG>

- 똑같은 상황에서 Pod가 pod4로 직접 연결을 하고 싶음  
  -> 서비스를 Headless 서비스로 만들어야 한다.  
  만드는 방법 : `clusterIP:None`이라고 지정하면 된다.

      pod4를 만들 때 `hostname`이라는 속성의 도메인 이름을 넣어야 하고, `subdomain`에 headless 서비스 명을 넣어야 한다.

<img src=../image/service1-5.PNG>

ClusterIP와는 다르게 Service의 IP가 없으므로 pod의 ip가 들어갔고,
pod이름의 첫 번째가 파드 IP가 아닌, 생성할 때 지정했던, hostname, subdomain으로 이루어졌기 때문에, pod가 직접 연결을 할 수가 있다.

## 1-2. Endpoint

---

<img src=../image/service1-6.PNG>

- 서비스와 파드를 연결할 때 라벨을 통해서 연결을 한다.
  -> 이것은 사용자 입장에서 둘의 연결을 하기 위한 도구이다.

- K8S는 서비스와 파드가 라벨을 통해 매칭이 됐을 때, Endpoint라는 것을 만들어주고 실제 연결고리를 관리를 한다.

- K8S가 Endpoint를 만드는 방법.
  서비스의 이름과 동일한 이름으로 엔드포인트 이름을 설정하고, 엔드포인트 안에는 파드의 접속 정보를 넣어준다.

-> 이 원리를 알면 LABEL이 없어도 직접 연결을 할 수가 있다.

하지만 IP는 언제든지 변경될 수 있기 때문에, 도메인이름으로 접근을 하는 방법을 알아야 하는데, 그것이 `ExternalName`이다.

## 1-3. ExternalName

---

<img src=../image/service1-7.PNG>

서비스의 externalName이라는 속성을 달아서, 도메인 네임을 넣을 수가 있는데, 이러면 DNS 캐쉬가 내부, 외부 DNS를 찾아서 IP를 알아낸다.

결국, 파드는 서비스를 가리키고만 있으면, 서비스에서 필요시마다 해당 도메인 주소를 변경할 수가 있어서, 접속할 곳이 변경되더라도 파드를 수정하고 재배포할 일이 없어진다.

# 2. Volume

## 개요.

---

<img src=../image/volume1.PNG>

- 볼륨은 데이터를 안정적으로 유지하기 위해서 사용.
- 그렇기 때문에 실제 데이터는 k8s cluster와 분리가 돼서 관리가 된다.
- 이런 방식으로 관리할 수 있는 볼륨의 종류

  - 외부망 :
    - AWS , GCP, Azure
  - 내부망 :
    - 기본적으로 k8s에서 node들의 실제 물리적인 공간의 데이터를 만들 수 있는 `hostpath`, `local` 볼륨이 있다.
    - 온프라미스 스토리지 솔루션을 별도로 설치할 수 있다.
      StorageOS, ceph, GlusterFS 와 같은 종류가 있고, 이 솔루션들이 알아서 노드의 자원을 이용해 볼륨을 관리한다.
    - NFS를 사용해서 다른 서버를 볼륨서버로 사용할 수 있다.
    - ETC...

- K8S Cluster 밖에 실제 볼륨들이 마련되어 있다면, 관리자는 pv를 만들고, 볼륨을 선택해서 연결을 한다.
- 그리고 사용자는 원하는 용량과 accessmode로 pvc를 만들면 k8s가 알아서 적절한 pv와 연결을 해준다.
- 이 pvc를 pod에서 사용을 한다.

이런 방식으로 볼륨을 사용하면, 볼륨이 필요할 때 마다 pv를 만들어줘야하고, 원하는 pv랑 연결을 하기 위해서 storage나 acessmode를 설정해야 한다. ( 복잡 )

-> K8S에서 `Dynamic Provisioning`해서 사용자가 pvc를 만들면 알아서 pv를 만들어주고 실제로 볼륨을 연결해주는 기능이 있다.

---

모든 pv에는 pod처럼 각각의 상태가 존재하므로, pv가 pvc와 연결되어 있는지, 에러가 났는지, 등등 알 수가 있다.

---

## 2-1. Dynamic Provisioning

---

<img src=../image/volume1-1.PNG>

- StorageOS 솔루션을 예시로 설명
- StorageClass를 사용해서 동적으로 pvc를 만들 수가 있다.
- PVC에 `StorageClssName`이라는 속성을 통해 StorageClass를 지정할 수 있다.
- IF ) PVC에 fast 이름을 지정하면 자동으로 StorageOS 볼륨을 가진 pv가 만들어진다.
- StorageClass는 추가로 만들 수 있고, `default`라는 것을 설정할 수 있는데, pvc에서 storageClassName을 생략을 해도 default StorageClass가 적용이 돼서 pv가 만들어 진다.

## 2-2. Status, ReclaimPolicy

---

<img src=../image/volume1-2.PNG>

- Status는 최초 pv가 만들어졌을 대 `available` 상태, PVC와 연결이 되면 `Bound` 상태
- PV를 직접 만드는 경우에는 아직 볼륨의 실제 데이터가 만들어진 상태가 아니고, 파드가 PVC를 사용해서 구동이 될 때 실제 볼륨이 만들어진다.
  -> 파드가 구동되다 삭제가 되는 경우에는 PVC와 PV에는 아무런 변화가 없기 때문에, 데이터에 문제는 없다.
  -> PVC를 삭제를 해야 PV가 `Released` 상태로 변화한다.
- 이러한 과정 중에 PV와 실제 데이터 간 연결의 문제가 생기면 `Faild`로 변하기도 한다.

PVC가 삭제가 되었을 때 PV에 설정해 놓은 ReclaimPolicy 따라서 PV에 대한 상태가 달라진다.
ReclaimPolicy는 : `Retain, Delete, Recycle` 3가지가 있다.

- Retain : PVC가 삭제가 되면 PV는 Released가 되는데, PV를 만들 때 ReclaimPolicy를 별도로 설정하지 않았을 경우 Default 값이다.
  - 실제 볼륨의 데이터는 유지되지만, PV를 다른 PVC에 연겨을 할 수는 없다.
  - 삭제하고 다시 생성 후 연결해야 한다.
- Delete : PVC를 지우면 PV도 같이 지워진다.
  - StorageClass를 사용해서 자동으로 만들어진 PV의 Default 값이다.
  - 볼륨의 종류에 따라 실제 데이터가 삭제되기도 하고 안되기도 한다.
- Recycle : PV의 상태가 Available이 되면서 PVC에서 다시 연결할 수 있는 상태이다.
  - 지금은 Deprecated 이다.

# 3. Accessing API

- <b> 개요 </b>
  <img src=../image/accessapi1.PNG>

- 마스터 노드에 쿠버네티스 api server가 있음
- api server를 통해서만 쿠버네티스의 자원을 만들거나 조회를 할 수 있다.  
  kubectl을 이용해서 cli를 통해 쿠버네티스 자원을 조회할 수 있는 것도, api server에 접근을 해서 정보를 가져오는 것이다.
- 내부에서는 이런식으로 접근이 가능함. 하지만 외부에서는
- 외부에서는 api-server에 접근을 하려면 인증서를 가지고 있는 사람만 https로 보안 접근을 할 수 있다.
- 하지만, 내부 관리자가 kubectl로 proxy를 열어줬다면, 인증서 없이 http로 api-server로 접근할 수 있다.
- kubectl은 외부에서도 사용할 수 있는데, `config` 기능을 활용하면, 쿠버네티스 클러스터가 여러 개 있을 때, 간편한 명령어를 통해서 내가 접근할 수 있는 클러스터에 연결상태를 유지할 수 있다.
- 연결이 된 상태에서는 `kubectl get` 명령으로 해당 클러스터 파드 정보들을 가져올 수 있다.  
  -> 이러한 접근 방법들은 USER 입장에서 API-server에 접근을 하는 방법이고, User Account라고 한다.

- Pod 입장에서 api 서버에 마음껏 접근을 할 수 있다면, 보안상 위험이 존재한다.
- k8s는 Service Account라고 해서 파드들이 api-server로 접근하는 방법이 있다.

즉, user 입장인 User Account, pod 입장인 Service Account 두 가지가 있다.
service-account도 사용방법을 알면 외부에서도 사용해 접근방법으로 사용할 수 있다.

-> 여기까지 쿠버네티스의 api-server에 접근을 하는 Authentication이다.

접근을 한 다음에, 내가 필요한 자원을 조회할 수 있는 권한도 필요하다.  


- NS가 2개 있고 파드가 분리되어 있음.
- 파드가 API 서버에 접근이 가능하더라도, API 서버가 파드를 조회할 수 있을까?
- 권한 여부에 따라 가능할 수도, 못할 수도 있다.

-> Authorization 대한 부분

- 권한까지 문제가 없다면
- Admission Control이라고 해서 관리자가 pv를 만들 때 용량을 1G 이상으로 만들지 못하게 제한했다면
- pod를 만들라는 api 요청이 들어왔을 때, 쿠버네티스는 설정된 크기를 넘지 못하도록 체크를 해야한다.

-> Admission Control

## 3-1 Authentication

---

<img src=../image/aceesingapi1-1.PNG>

### X509 Client Certs

---

- cluster에 6443포트로 api server가 열려있음.
- 사용자가 api-server에 https 접근을 하려면?
- 쿠버네티스 설치 시 `kubeconfig`라고 해서 클러스터에 접근을 할 수 있는 정보들이 들어 있는 파일이 있다.
- 파일 안에는 인증서 내용이 있으므로 client key 인증서를 복사해서 가져오면 된다.

---

<img src=../image/aceesingapi1-2.PNG>
CA key : 발급기관 개인키
CA csr : 인증요청서

Client Key : 클라이언트 개인키
Clinet csr : 인증요청서

1. 최초로 CA key, Client Key를 생성한다.
2. 개인키를 가지고 인증서를 만들기위한 각각의 인증 요청서인 csr을 만든다.
3. CA 경우, CSR을 통해 바로 인증서를 만드는데, kubeconfig에 있는 CA crt가 이것이다.
4. Client 경우, 발급 기관의 key, 인증요청서, Client 인증 요청서를 가지고 인증서를 만든다.
5. 이렇게 만들어진 client.crt는 kubeconfig에 들어있고, client.key도 들어 있다.

---

- 쿠버네티스를 설치할 때 kubectl을 설치하고, 설정 내용 중에 kubeconfig 파일을 통째로 kubectl에서 사용하도록 복사하는 과정이 있기 때문에, user는 kubectl로 쿠버네티스 api에 인증이 돼서 자원을 조회할 수 있다.

- `accept-hosts`옵션을 통해서 8001번 포트로 proxy 열어두면, 외부에서도 http로 접근을 할 수 있게되는데, kubectl이 이미 인증서를 가지고 있기 때문에 사용자는 아무런 인증서 없이 사용할 수 있게 된다.

### Kubectl

---

<img src=../image/aceesingapi1-3.PNG>

- 위 사진은 외부 서버에 kubectl을 설치해서, 멀티 클러스터에 접근을 하는 내용이다.
- 사전에 각 클러스터의 kubeconfig 파일이 kubectl에 존재해야 한다.
- 그래야만 원하는 클러스터에 접근을 해서 자원을 조회 및 생성할 수 있다.

---

Kubconfig
<img src=../image/aceesingapi1-4.PNG>

1. clusters 항목으로 cluster를 등록할 수 있고, 내용으로는 name, url, CA 인증서가 있다.
2. users 항목으로 사용자를 등록할 수 있고, user name, user에 대한 개인키와 인증서가 있다.
3. contexts 항목을 통해서 위 둘을 연결할 수 있다. 내용으로는 text 이름, 연결할 cluster, user name이 있다.
4. 여러 개도 가능하다.

---

- kuebconfig를 만들었을 때 `kubectl config user-context context-A` 형식으로 현재 사용하고 싶은 context를 지정할 수 있고, `kubectl get nodes` 명령을 치면 cluster-a의 정보가 뜬다.

### Service Account

---

<img src=../image/aceesingapi1-5.PNG>

- 쿠버네티스 클러스터와 api server가 존재하고, ns를 만들게 되면 기본적으로 default라는 service account가 자동으로 만들어진다.
- service account에는 secret이 달려 있는데, 내용으로는 ca.crt 정보와, token 값이 있다.
- 파드를 생성하면 service account가 연결이 되고, 파드는 token 값을 통해서 api-server에 연결을 한다.
- 결국, token 값만 알면 사용자도 api-server에 접근을 할 수가 있다.

## 3-2. Authorizaition

---

<img src=../image/authorization1.PNG>

### RBAC Overview

---

- 쿠버네티스에는 node, pv, ns와 같이 클러스터 단위로 관리되는 자원과
  pod와 service 같이 ns 단위로 관리되는 자원으로 나눌 수 있다.

<b>< NameSpace 단위 ></b>

- ns를 만들면 자동적으로 service-account가 만들어지기도 하고, 추가적으로 더 만들 수도 있다.
- service-account에 Role과 RoleBinding을 어떻게 설정하느냐에 따라 해당 service-account는 ns 내에 있는 자원에만 접근할 수 있고, 클러스터에 있는 자원에도 접근을 할 수 있다.

- Role은 여러 개를 만들 수 있고, 각 Role에는 ns 내에 있는 자원에 대해서 조회만 가능하거나, 생성만 가능하게 권한을 줄 수도 있다. ( 여러 가지 case )
- RoleBiniding은 Role과 service-account를 연결해주는 역할인데, Role은 하나만 지정할 수 있고, service-account는 여러 개를 지정할 수 있다.

- EX ) service-account, service-account1은 Role1에 지정되어 있는 권한으로 api-server에 접근을 할 수 있다.

-> 즉, 한 NS 안에 권한을 부여해서 관리할 때는 Role과 RoleBinding을 여러 개 만들어서 관리하면 된다.

<b>< Cluster 단위 ></b>

- ClusterRole과 ClusterRoleBinding이 만들어져 있어야 한다.
- ClusterRole은 Cluster 단위의 오브젝트들을 지정할 수 있다는 것이 Role과의 차이점이다. (기능은 똑같음)
- ClusterRoleBinding에 Service-account를 추가하게 되면 NS-A에 있는 Service-account에서도 Cluster 자원의 접근을 할 수 있는 권한을 얻게 된다.

- NS 내의 RoleBiniding이 Service-account와 연결이 됐고, Role을 지정할 때, NS 내의 Role이 아닌 ClusterRole을 지정을 할 수도 있다.
- 이렇게 되면, Service-account는 Cluster 자원에는 접근을 하지 못하고, 자신의 NS 내의 자원만 사용할 수 있다.
- 그럼 결국, 그냥 Role을 만들어서 사용하는 것과 같은데, ClusterRole에 연결하는 이유는?
- 모든 NS마다 똑같은 Role을 부여하고 관리를 해야되는 상횡에서 각각의 NS 마다 똑같은 Role에 대한 내용을 만들게 되면, 이 Role에 대한 수정이 필요할 경우, 모든 Role을 수정을 해야한다. -> 관리하기 어려움

- ClusterRole을 하나 만들어 두고, 모든 NS의 RoleBinding이 ClusterRole을 지정할 경우, 권한에 대해 변경사항이 있을 때 하나만 수정하면 되기 때문에 관리가 쉬워진다.

### Role, RoleBinding Detail

---

<img src=../image/authorization2.PNG>

- 한 NS에 Pod와 Service가 있고, NS를 만들면 자동으로 생성되는 Service-account와 token 값이 있는 Secret도 있다.
- 위 환경에서 Role을 만들었을 때, 내용으로는 apiGroups, resources(pod일 경우), verbs(조회만 가능)

---

pod는 core-api이기 때문에 apiGroups에 값이 안들어 가도된다.
만약 resources가 job일 경우네는 apiGroups에 batch가 들어간다.

---

- RoleBinding을 만들고, `rolRef` 속성에 Role을 연결하고, `subjects` 속성에 Service-account를 연결하면 끝난다.
- 이렇게 연결하면, secret token 값을 가지고, api-server에 접근을 할 수 있고, token 권한에 따라 NS 내에 있는 POD만 조회할 수 있게 된다.


- 또 다른 NS가 있고, Service-account를 admin의 권한처럼 모든 cluster자원에 접근을 할 수 있도록 만들려면?
- ClusterRole을 모든게 가능하도록 만듬.
- 이후, ClusterRoleBinding을 만들어서 Service-Account를 연결하면 된다.
- 연결이 되면, token으로 api-server에 접근을 하면, 다른 NS에 있는 자원은 물론이고, Cluster 단위의 자원도 조회 및 생성이 가능하다.

## 4. Dashboard

---

1. 1.10.1

   <img src=../image/dashboard1.PNG>
   kube-system namespace에 Deployment로 dashboard pod가 생성이 되고, service와 연결이 되어있다. 설치시에 kubectl 명령으로 proxy(8001)을 올려놨기 때문에 사용자는 proxy를 통해서 보안 없이 http api로 dashboard에 접근이 가능한 것이다.

로그인을 하게 되면 쿠버네티스 자원들을 조회하고 생성할 수 있는데, 이것은 모두 pod가 api-server에 접근을 했을 때 cluster의 대한 권한이 있었기 때문에 가능했던 것이다.

파드는 service-account가 연결되어 있고, service-account는 RoleBinding을 통해서 Role이 연결이 되어 있는데, Role에 대한 권한은 NS 내에서만 사용할 수 있기 때문에 Cluster 자원을 사용할 수 없다.

쿠버네티스 설치 시 자동으로 생성되는 cluster-admin 권한을 가진 ClusterRole이 있는데, 이것을 ClusterRoleBinding을 새로 만들어 연결해 Service-account와 연결시켰기 때문에 Cluster admin의 권한으로 api-server에 접근을 할 수 있던 것이었다.

-> 보안적으로 취약

2. 2.0.0
   <img src=../image/dashboard2.PNG>
   proxy 중간에 두지 않고, api-server로 바로 접근을 할 것이기 때문에 kubeconfig 파일에 있는 client.crt와 client.key를 합쳐서 client p12 파일을 만들어 local pc에 인증서 등록을 할 것이다.

https로 api-server에 접근을 할 수 있을 것이고, 2.0.0 version에 dashboard를 설치를 하면 마찬가지로 dashboard 관련 오브젝트들이 만들어져서 사용자는 api-server를 통해 dashboard에 접근을 할 수가 있게 되고, dashboard는 사용자가 필요한 자원을 조회하거나 만들 때마다 api-server로 해당요청을 하게 된다.

2.0.0 버전에는 추가적으로 생성되는 pod가 있는데, 만약 사전에 metrics-server가 설치가 되어 있다면, node나 pod의 성능 정보를 그래프로 dashboard에 표시될 수 있도록 역할을 한다.

이때, 같이 생기는 ClusterRolebinding과 ClusterRole은 이 파드에 노드나 파드에 대한 조회 권한을 주기 위한 것이다.

여기에, 이전 버전과 마찬가지로 기존에 존재하는 ClusterRole과 Service-account와 연결을 하고, service-account와 연결된 secret의 token 값으로 dashboard에서 token 로그인을 할 수 있어서 보안성이 높아진다.
