# Kubernetes Architecture

<img src=../image/arch1.PNG>

## Components

<img src=../image/arch2.PNG>

Master Node에는 etcd와 kube-scheduler, kube-apiserver가 있다.
일반적인 설치를 했을 때 이 Component들은 pod의 형태로 올라와 있다.

master node에 `/etc/kubenetes/manifests` 폴더를 보면 Component의 생성에 대한 yaml 파일이 있다. kubenetes가 기동 시에 이 파일들을 읽어서 이 파드들을 static으로 띄운다.

Worker Node에는 kubelet과, container Runtime이 있다.

가정 :
사용자가 kubectl 명령으로 pod 생성 요청을 날렸음.

1. 파드 생성 명령은 kube-apiserver로 전달
2. api-server는 etcd에 파드에 대한 입력정보들을 저장한다.
3. kube-scheduler는 api-server에 pod 생성 요청이 있는지 확인하고, 노드 자원 상태를 확인하고, 어디가 좋은지 판단하고, 파드에 노드 정보를 붙인다.
4. kubelet도 kube-apiserver를 보면서, 파드에 자신의 노드 정보가 있는지 없는지 체크하다가, 파드ㅔ 자신의 노드 정보가 있다면, 정보를 가져와서 파드를 만들게 된다.
5. container runtime에게 contaienr 생성 요청을 하고 container runtime은 container를 만들어준다.
6. kubelet은 kube-proxy에게 network 생성 요청을 하고, kube-proxy는 새로 생성된 container에 통신이 되도록 도와준다.

## Networking

<img src=../image/arch3.PNG>

클러스터를 설치할 때 `pod-network-cidr`로 설정한 pod network에 대한 영역이 있음.

### pod networking이란.

---

- pod 안 container 간에 네트워킹을 하는 부분인데, 파드가 생성이 되면, 파드 네트워크 범위 안에서 고유 ip를 가지는 인터페이스가 생기는데, 이걸 가지고 여러 컨테이너 간에 어떻게 통신이 되는지 설명할 것
- 또 다른 파드가 생성이 됐을 때 두 파드 간에 통신은 쿠버네티스에서 노드마다 설치가 되는 네트워크 플러그인에 인해서 통신이 되는 과정을 설명할 것

쿠버네티스에는 기본적으로 제공하는 kubenet이라는 네트워크 플러그인이 있는데, 기본적으로 제한적인 부분이 많기 때문에 잘 사용하지 않고, CNI라고 해서 이것을 통해 다양한 오픈소스 네트워크 플러그인을 설치할 수 있다.

Network 플러그인이 하는 역할은 같은 노드 위의 파드들 간의 통신과 외부 네트워크를 통한 타 노드위에 있는 파드들 간의 통신을 담당한다.

### Service Networking이란.

---

쿠버네티스를 설치할 때 별도 옵션을 주지 않아서 default 값으로 10.96.0.0/12가 설정이 될 것이다.
이렇게 파드에 서비스를 붙이게 되면, 서비스도 고유 ip가 할당이 되고, 서비스 생성과 동시에 kube-dns에는 서비스 이름과 ip에 대한 도메인이 등록이 된다. 그리고 api-server가 worker 노드들 마다 kube-proxy에 서비스의 ip가 어느 ip와 연결이 되어 있는지에 대한 정보를 전해준다.

이제 여기서 서비스의 ip를 파드의 ip로 변경해주는 NAT 기능이 필요한데, kube-proxy가 iptables나 ipvs를 어떻게 이용하느지에 따라서 Proxy Mode라고 해서 작동방식이 3가지가 있다.

이 상태에서 파드가 서비스 이름을 호출하게 되면, kube-dns를 통해 ip를 알게되고 얻은 서비스 ip를 nat 영역으로 가져오게 되는데, 여기에 해당 서비스에 대한 pod 매핑 정보가 있기 때문에, 서비스는 네트워크 플러그인을 통해 해당 파드로 가게된다.

결국 서비스 오브젝트 실체는 nat 영역 내의 설정이고, 서비스를 삭제하게 되면, api-server가 이것을 감지하고, kube-proxy에게 이 내용을 지우라고 한다.

## Pod Network

### Pause Container

---

<img src=../image/arch4.PNG>
파드를 만들게 되면 pause container라고 해서 네트워킹을 담당하는 컨테이너가 자동적으로 생성이 된다. 이 컨테이너에 인터페이스가 달려있고, ip도 생기는데, 쿠버네티스가 이 pause 컨테이너에 네트워크 네임스페이스를 파드 내의 모든 컨테이너들이 같이 쓰도록 구성한다.

이렇게 되면 ip에 대한 네트워크를 모두 공유하게 되고, 컨테이너 간의 구분은 포트를 통해 하게 된다.

한편, worker 노드에 host 네트워크 네임스페이스가 있고 호스트 ip 인터페이스가 있는데, pause 컨테이너가 생기면 이 호스트 네임스페이스에 가상 인터페이스가 생기고 pause 컨테이너 인터페이스와 연결이 된다. ( 1:1 매칭 )

### Network Plugin

---

<img src=../image/arch5.PNG>

calico cni를 사용했을 때 host network에 생성된 가상 인터페이스가 바로 router에 연결되는 구조이고, 두 파드 간에 통신은 라우터가 해준다. 라우터 윗단에니는 overlay 네트워크를 제공해주는 층이 있는데, 종류는 IPIP와 VXLAN 방식이 있다. 오버레이 네트워크기 하는 일은 노드 위에 있는 파드가 타 노드 위에 있는 파드와 통신이 되도록 해준다.

Calico cin를 사용했을 때 workernode 위에 있는 파드들이 통신하는 과정 흐름.

podd에서 podb로 트래픽을 날린다고 했을 때, 파드 d에서는 파드 b의 ip인 20.111.156.7로 호출을 하면, 그대로 라우터에 있는 가상 인터페이스를 지나 해당 ip는 이 라우터 안에 없기 때문에, 오버레이 층으로 올라간다. calico는 이 호출된 파드의 ip 대역이 어느 노드에 있는지 알고 있기 때문에 오버레이 네트워크 층에서 패킷을 해당 노드의 ip로 변경해주는데 이때, 실제 pod ip는 캡슐화 된다. 이 트래픽은 192.168.59.22로 가게 되고, 노드 1번에 들어온 트래픽이 오버레이 네트워크 층에서 decapsulation 되면서 원래의 pod ip로 변환된다. 그리고 ip 대역이 있는 라우터로 트래픽이 전해지고, 라우터 안에서 해당 ip에 맞는 가상 인터페이스를 찾아 최종적으로 요청한 pod까지 도달하게 된다.

## Service Network

### Proxy Mode

---

<img src=../image/arch6.PNG>

쿠버네티스에서 제공해주는 proxy mode는 3가지가 있다.

1. Userspace Mode
2. Iptables Mode
3. IPVS Mode

이 3가지 모드를 사용하기 위해서 필요한 전제 조건:

파드를 만들고 서비스를 붙일 때, 파드가 정상이라면 중간에 endpoint라는 오브젝트가 생성이 돼서 실제 연결상태를 담당한다.

서비스의 ip는 `Service Network CIDR` 범위 안에서 생성이 되는 것이고, 이때 api-server는 endpoint를 감시하다가 kube-proxy에게 service의 ip는 pod의 ip로 포워딩 된다는 것을 알려준다.

- UserspaceMode
  worker 노드에 기본적으로 설치된 iptables에 service CIDR로 들어오는 트래픽은 모두 kube-proxy에게 주도록 설정이 되어있는 상태이고, 만약 파드에서 서비스의 ip를 호출할 경우, 이 트래픽은 iptables를 거쳐 kube-proxy로 가게 된다.
  kube-proxy는 자신이 가지고 있는 매핑 정보를 보고, 트래픽을 pod ip로 바꿔준다. 그래서 이렇게 파드ip로 변경된 트래픽은 그 다음부터 파드 네트워크 영역으로 통신이 된다.

단점 : 모든 트래픽이 kube-proxy로 거쳐서 들어가는데, kube-proxy의 성능이 그렇게 좋지 않아서 자주 쓰이지 않는 mode이다.

- IPtablesMode(default)
  kube-proxy가 ip 매핑 정보를 iptables에 직접 등록하는 방식이라, 파드에서 보내는 서비스 ip는 iptables에서 직접 파드ip로 변경된다.

- IPVSMode
  리눅스에서는 IPVS라고 해서 L4 로드밸런서라는 것을 제공을 하는데,
  서비스 모드를 IPVS 모드로 설정을 하면 IPTABLES와 같은 역할을 한다.

### Service Type

---

서비스를 만들 때 클러스터 IP나 노드 IP 타입에 따라 트래픽 흐름이 다르다.

<img src=../image/arch7.PNG>

- ClusterIP
  라우터 부분에서 서비스 IP를 POD IP로 변환해주는 NAT 역할을 하는 기능이 있다. 파드 D에서 파드 B로 트래픽을 날린다고 했을 때, 파드 B에 ClusterIP 타입의 서비스를 연결을 하고, 파드 D에서 서비스 ip로 트래픽을 날리게 되면, NAT에서 해당 서비스 IP와 매칭되는 파드 IP로 변경이 된다. 이후는 위의 Network Plugin에서 진행되는 과정과 같음.

<img src=../image/arch8.PNG>

- NodePort
  모든 노드에 있는 kube-proxy가 자신의 노드에 30000번대 노드포트를 열어주고, 외부에서 hostip:포트로 트래픽이 들어오게 되면, iptables 에서 트래픽을 calico cni로 보내게 된다. 여기서는 이제 ClusterIp와 같이 진행된다.

# Storage

<img src=../image/storage1.PNG>

일단 PV를 만드는 방법에는 여러가지가 있다.

- 일반적으로 pv를 만들고, pvc를 만드는 것
- stroageClass를 통해 Grouping 하는 방법
- StorageClass를 통해 동적으로 pv를 만드는 방법.

PV는 기본 속성 중 `capacity`라는 용량을 지정하는 속성, `accessModes`를 지정하는 부분이 있고, `Volume Plugin`이라고 해서, PV의 실체를 결정하는 부분도 있다.

## Volume Plugin

Volume Plugin의 종류를 보면 대표적으로 hostPath가 있다.
pv를 hostPath로 만들면 worker node에 지정된 path를 pv에 대한 volume으로 사용한다는 것이다.

외부 서버에 nfs 서버를 구성이 되어 있어서 사용을 원한다면 `NFS`로 설정을 하면 된다. 쿠버네티스가 os에 설치되어 있는 nfs client기능을 이용해서 설정이 가능한 것인데, 만약 os에 설치가 안되어 있다면 사용 불가능하다.

Cloud Service의 볼륨에 연결을 하고 싶다면, 해당 서비스에 맞는 볼륨 플러그인이 있고, 클라우드 서비스 뿐만 아니라 자신들의 볼륨 솔루션을 가진 회사들이 있는데, 이것들 또한 pv를 통해 해당 서비스와 연결이 가능하다.

마지막으로 CSI에 대해서 설명할 것인데, 만약 미리 자신의 볼륨 솔루션을 미리 쿠버네티스에 반영을 하지 않았다거나, 반영을 했더라도 볼륨의 최신 버전이 나왔을 때 그 버전을 바로 적용을 할려면 CSI를 사용하면 된다.

쿠버네티스 문서 중에 CSI에 대한 가이드 문서가 있다. 기업들은 그 가이드에 따라 CSI 플러그인이나 프로비저너를 만들어 두고 설치 방법만 사용자에게 안내를 해놓으면, 필요한 사용자들은 그것을 자신의 클러스터에 반영을 할 수 있다.

뿐만 아니라, 볼륨 솔루션 조차도 기존 볼륨 시스템과 연결을 하는 구조가 아닌, 새롭게 컨테이너로 볼륨 시스템 자체를 띄울 수 있도록 해놓은 기업들이 많다.

## AccessMode

<img src=../image/storage2.PNG>

각각 볼륨들 마다 storage type이 있는데, nfs는 fileStorage, CloudService는 Block,File,Object Storage 등 지원을 하고, 3rd Party Vendors는 FileStorage를 지원을 하고, CSI는 각자의 특성에 맞는 storage type을 제공을 한다.

(해당 가이드에 맞게 사용하면 된다.)

FileStorage와 ObjectStorage는 RWO를 지원하기 때문에 한 NODE 위에 여러 파드와 연결을 할 수 있고, RWM, ROM도 지원하기 때문에 다른 노드 위에 있는 파드들에서도 연결이 된다.

BlockStorage는 기술적으로 특정 한 node에 storage를 마운팅하는 개념이기 때문에, 특정 노드 위에 파드들과 연결되는 것은 문제가 없지만, 다른 노드 위에 파드와는 연결이 불가능하다.

그렇기 때문에 BlockStorage를 사용한다는 건 PV의 accessMode를 사용할 때 RWO만 가능하다는 것을 알아야 한다.

일반적으로 FileStorage와 ObjectStorage는 공유 데이터용으로 사용하고, 그 중 ObjectStorage는 백업용이나 이미지 저장용으로 사용이 되고, 이러한 특징으로 Deployment pv를 붙일 때 사용이 된다.
BlockStorage는 빠른 ReadWrite가 가능하기 때문에DB 데이터용으로 주로 사용이 된다. 또한, statefulset은 파드를 만들 때마다 pvc를 새로 만들기 때문에 BlockStorage의 특성과 잘 맞아서 같이 사용한다.

## 실습할 때 내용

<img src=../image/storage3.PNG>

대표적인 fileStorage인 nfs 서버를 만들 것이고, pv를 만들어서 연결을 한다음 selector와 label을 통해 pvc와 연결을 할 것이다. 또한, 두 노드에 각각 파드를 만들어서 pvc에 붙인 후 파일 스토리지를 통해 파일 공유가 잘되는지 확인을 할 것이다.

<img src=../image/storage4.PNG>

longhun?에 대한 내용. 필요하다면 정리할 것.

# Logging

쿠버네티스에서 말하는 Logging이란, app의 log 내용이고,
monitoring은 cpu나 memory와 같은 자원을 이야기 한다.

이런 기능들에 대해서 쿠버네티스 자체적으로 제공해주는 core pipeline이 있고, 추가적으로 플러그인을 설치해서 기능을 더 강화시킬 수 있는 service pipeline이 있다.

<img src=../image/logging1.PNG>

쿠버네티스 core pipeline을 먼저 보면, kubelet에 의해 pod가 생성되고, container는 runtime에 의해 생성이 된다. runtime이 container를 만들 때 worker node의 disk를 사용하면서 data를 쓰고 log를 작성한다. 뿐만 아니라 서비스가 기동 되면서 cpu와 mem 자원도 사용을 하게 된다.

이러한 상황에서 monitoring에 대한 corepipline은 각 노드에 있는 kubelet은 별도로 설치되어 있는 cAdviser를 통해 도커로부터 cpu, mem 정보를 가져올 수 있고, 각 노드의 monitoring 정보는 metric server로 모아진다. 사용자는 `kubectl top`명령어를 통해 이러한 정보를 조회할 수 있고, `kubectl log`명령어를 통해 container의 log도 조회할 수 있다.

전반적인 이런 형태가 쿠버네티스가 제공하는 corepipeline의 흐름이다.

별도의 플로그인을 설치하는 service 파이프라인은 kubelet처럼 모든 node에 설치가 돼서 데이터를 가져오는 역할을 위해 daemonset을 이용한 agent 영역이 있고, 이 agent가 cAdvisor나, container runtime, worker node를 통해 log와 resource 자원을 수집하는 역할을 한다.

그리고 별도의 worker node에 metrics server와 같이 각각의 노드의 metric들을 수집하고 분석하는 서버 영역이 있다. 이 서버에는 많은 데이터가 저장되기 때문에 별도 저장소를 구성하는 것을 권장하고 있다.

마지막으로 web ui가 수집 서버로 qurey하면서 필요한 정보를 사용자에게 보여주는 것이 service 파이프라인의 기본적인 구성이다.

대표적인 오픈소스로, elastic, loki, prometheus가 있다.

## Logging Detail

### Node-level Logging

---

<img src=../image/logging2.PNG>

- 단일 노드 안에서 일어나는 logging 구조.

파드 내 컨테이너에서 app 자체의 log를 /log/app/app.log로 생성한다고 가정.
컨테이너는 도커가 만들기 때문에, 도커의 logging driver를 통해 log가 생성이 되는데, `/etc/docker` 안에 `daemon.json`에 들어 있다. 내용을 보면, log를 쌓을 때 컨테이너 당 최대 3개의 파일이 생기고, 각 파일은 100m만 생성이 되도록 설정한 것이다.

이렇게 설정을 해도, app에 있는 log가 바로 쌓이는 것이 아니기 때문에 설정을 해줘야 한다.
log를 꼭 stdout, stderr로 출력을 해야하고, 각 언어 마다 방법이 다르기 때문에 알아봐야 한다.

이렇게 출력한 log는 `/var/lib/docker/` path에 `container` 폴더 안에 `[contaier ID]` 컨테이너 이름 별로 만들어지면서 log가 쌓이게 된다.

-> 위의 과정은 docker 자체적으로 docker driver를 통해 컨테이너 로그가 worker node에 만들어지는 과정이다.

쿠버네티스는 `/var/log`폴더에 `pods` 폴더를 만들고 특정 이름 규칙에 따라 도커에서 만든 log파일에다 링크를 걸어준다.
이 링크된 파일을 가지고 `containers` 폴더에 이러한 name 규칙을 가지고 파일을 링크한다.

이 상태에서 파드를 삭제를 하게되면, container도 날아가기 때문에, `/var/lib/docker/contaier` 안에 있는 해당 이름 파일도 사라지게 되는데, 이러면 링크된 파일들은 모두 사라진다.

다시 말해, node-level logging은 pod가 살아있는 동안만 유지가 되는 것이고, `kubectl log` 명령으로 pod의 현재 실시간 log를 조회할 수는 있지만 죽게 되면 볼 수 없다.

### Cluster-level Logging

---

<img src=../image/logging3.PNG>

- 쿠버네티스 클러스터 안에 모든 노드들에 있는 log들을 포괄하는 logging 구조

node-level logging은 pod가 죽으면 log를 볼 수 없지만 cluster-level은 다르다.
하지만 이것은 쿠버네티스에서 기능을 제공하지 않고, 아키텍쳐만 제시를 해준다.
